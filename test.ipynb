{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from matplotlib import pyplot as plt\n",
    "import yaml\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'N', 'Q', 'G', 'O', 'I', 'L', 'M', 'F', 'P', 'U', 'S', 'T', 'W', 'V', 'D', 'C', 'E', 'Y', 'R', 'H', 'K']\n"
     ]
    }
   ],
   "source": [
    "from generator import Generator\n",
    "set_file = \"sets.yml\"\n",
    "with open(set_file, 'r') as outfile:\n",
    "    setf = yaml.safe_load(outfile)\n",
    "sets = setf[0]+setf[1]+setf[2]\n",
    "print(sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1195659559.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[18], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    continue()\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "gen = Generator()\n",
    "for k in tqdm( range(20000)):\n",
    "    combine = []\n",
    "    coms = []\n",
    "    com = ''\n",
    "    tokens_pc = ''\n",
    "    datainput = ''\n",
    "    rand_len = np.random.randint(2,5)\n",
    "    for i in range(0, rand_len):\n",
    "        rand_idx = np.random.randint(len(sets))\n",
    "        combine.append(sets[rand_idx])\n",
    "        com = com+sets[rand_idx]+' '\n",
    "    combine.append('<EOS>')\n",
    "    if com in coms :\n",
    "        continue\n",
    "    coms.append(com)\n",
    "    tokens_pot, tokens_cap = gen.get(combine)\n",
    "    #print(tokens_pot, tokens_cap)\n",
    "    if len(tokens_cap) != 0:\n",
    "        tokens_cap = tokens_cap[:-1]\n",
    "    for i in tokens_pot:\n",
    "        tokens_pc = tokens_pc + str(i) +' '\n",
    "    for i in tokens_cap:\n",
    "        tokens_pc = tokens_pc + str(i) +' '\n",
    "\n",
    "    com= com[:-1]\n",
    "    tokens_pc = tokens_pc[:-1]\n",
    "\n",
    "    datainput = com + '\\t' + tokens_pc + '\\n'\n",
    "    \n",
    "\n",
    "    #print(datainput)\n",
    "\n",
    "    with open('bigdata.txt', 'a') as file:\n",
    "        file.write(datainput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mydata.txt') as f:\n",
    "    sentences = f.readlines()\n",
    "# print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 42850.22it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 30506.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[235, 22, 107, 413, 404, 441, 28, 423, 296, 296]\n",
      "['<sos>', '362', '622', '881', '0', '331', '912', '<eos>', '<pad>', '<pad>']\n",
      "[20, 5, 16, 4, 9, 15, 15, 15, 15, 15]\n",
      "['<sos>', 'w', 'd', 'n', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_INSTANCES = 10000\n",
    "MAX_SENT_LEN = 10\n",
    "eng_sentences, deu_sentences = [], []\n",
    "eng_words, deu_words = set(), set()\n",
    "for i in tqdm(range(NUM_INSTANCES)):\n",
    "  rand_idx = np.random.randint(len(sentences))\n",
    "  # find only letters in sentences\n",
    "  eng_sent, deu_sent = [\"<sos>\"], [\"<sos>\"]\n",
    "\n",
    "  eng_sent += re.findall(r\"\\w+\", sentences[rand_idx].split(\"\\t\")[1]) \n",
    "  deu_sent += re.findall(r\"\\w+\", sentences[rand_idx].split(\"\\t\")[0])\n",
    "\n",
    "  # change to lowercase\n",
    "  eng_sent = [x.lower() for x in eng_sent]\n",
    "  deu_sent = [x.lower() for x in deu_sent]\n",
    "  eng_sent.append(\"<eos>\")\n",
    "  deu_sent.append(\"<eos>\")\n",
    "\n",
    "  if len(eng_sent) >= MAX_SENT_LEN:\n",
    "    eng_sent = eng_sent[:MAX_SENT_LEN]\n",
    "  else:\n",
    "    for _ in range(MAX_SENT_LEN - len(eng_sent)):\n",
    "      eng_sent.append(\"<pad>\")\n",
    "\n",
    "  if len(deu_sent) >= MAX_SENT_LEN:\n",
    "    deu_sent = deu_sent[:MAX_SENT_LEN]\n",
    "  else:\n",
    "    for _ in range(MAX_SENT_LEN - len(deu_sent)):\n",
    "      deu_sent.append(\"<pad>\")\n",
    "\n",
    "  # add parsed sentences\n",
    "  eng_sentences.append(eng_sent)\n",
    "  deu_sentences.append(deu_sent)\n",
    "\n",
    "  # update unique words\n",
    "  eng_words.update(eng_sent)\n",
    "  deu_words.update(deu_sent)\n",
    "\n",
    "eng_words, deu_words = list(eng_words), list(deu_words)\n",
    "\n",
    "# encode each token into index\n",
    "for i in tqdm(range(len(eng_sentences))):\n",
    "  eng_sentences[i] = [eng_words.index(x) for x in eng_sentences[i]]\n",
    "  deu_sentences[i] = [deu_words.index(x) for x in deu_sentences[i]]\n",
    "\n",
    "idx = 10\n",
    "print(eng_sentences[idx])\n",
    "print([eng_words[x] for x in eng_sentences[idx]])\n",
    "print(deu_sentences[idx])\n",
    "print([deu_words[x] for x in deu_sentences[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENG_VOCAB_SIZE = len(eng_words)\n",
    "DEU_VOCAB_SIZE = len(deu_words)\n",
    "NUM_EPOCHS = 20\n",
    "HIDDEN_SIZE = 16\n",
    "EMBEDDING_DIM = 30\n",
    "BATCH_SIZE = 128\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 3\n",
    "LEARNING_RATE = 1e-3\n",
    "DROPOUT = .3\n",
    "DEVICE = torch.device('cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_mask(inp, pad_idx=1):\n",
    "    return torch.triu(inp.new_ones(inp.size(1),inp.size(1)), diagonal=1)[None,None].byte()\n",
    "\n",
    "def compose(*functions):\n",
    "    return functools.reduce(lambda f, g: lambda x: f(g(x)), functions, lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self):\n",
    "    # import and initialize dataset    \n",
    "    self.source = np.array(eng_sentences, dtype = int)\n",
    "    self.target = np.array(deu_sentences, dtype = int)\n",
    "    \n",
    "  def __getitem__(self, idx):\n",
    "    # get item by index\n",
    "    return self.source[idx], self.target[idx]\n",
    "  \n",
    "  def __len__(self):\n",
    "    # returns length of data\n",
    "    return len(self.source)\n",
    "\n",
    "np.random.seed(777)   # for reproducibility\n",
    "dataset = MTDataset()\n",
    "NUM_INSTANCES = len(dataset)\n",
    "TEST_RATIO = 0.3\n",
    "TEST_SIZE = int(NUM_INSTANCES * 0.3)\n",
    "\n",
    "indices = list(range(NUM_INSTANCES))\n",
    "\n",
    "test_idx = np.random.choice(indices, size = TEST_SIZE, replace = False)\n",
    "train_idx = list(set(indices) - set(test_idx))\n",
    "train_sampler, test_sampler = SubsetRandomSampler(train_idx), SubsetRandomSampler(test_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size = BATCH_SIZE, sampler = train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size = BATCH_SIZE, sampler = test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## source: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerNet(nn.Module):\n",
    "  def __init__(self, num_src_vocab, num_tgt_vocab, embedding_dim, hidden_size, nheads, n_layers, max_src_len, max_tgt_len, dropout):\n",
    "    super(TransformerNet, self).__init__()\n",
    "    # embedding layers\n",
    "    self.enc_embedding = nn.Embedding(num_src_vocab, embedding_dim)\n",
    "    self.dec_embedding = nn.Embedding(num_tgt_vocab, embedding_dim)\n",
    "\n",
    "    # positional encoding layers\n",
    "    self.enc_pe = PositionalEncoding(embedding_dim, max_len = max_src_len)\n",
    "    self.dec_pe = PositionalEncoding(embedding_dim, max_len = max_tgt_len)\n",
    "\n",
    "    # encoder/decoder layers\n",
    "    enc_layer = nn.TransformerEncoderLayer(embedding_dim, nheads, hidden_size, dropout)\n",
    "    dec_layer = nn.TransformerDecoderLayer(embedding_dim, nheads, hidden_size, dropout)\n",
    "    self.encoder = nn.TransformerEncoder(enc_layer, num_layers = n_layers)\n",
    "    self.decoder = nn.TransformerDecoder(dec_layer, num_layers = n_layers)\n",
    "\n",
    "    # final dense layer\n",
    "    self.dense = nn.Linear(embedding_dim, num_tgt_vocab)\n",
    "    self.log_softmax = nn.LogSoftmax()\n",
    "\n",
    "  def forward(self, src, tgt):\n",
    "    mask_tgt = get_output_mask(tgt)\n",
    "    src, tgt = self.enc_embedding(src).permute(1, 0, 2), self.dec_embedding(tgt).permute(1, 0, 2)\n",
    "    src, tgt = self.enc_pe(src), self.dec_pe(tgt)\n",
    "    memory = compose(self.encoder)(src)\n",
    "    transformer_out = compose(self.decoder)(tgt, memory, mask_tgt)\n",
    "    final_out = self.dense(transformer_out)\n",
    "    return self.log_softmax(final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/postgrads/2650807G/spack/opt/spack/linux-centos7-x86_64_v3/gcc-4.8.5/anaconda3-2022.10-ldbv2ghkumdvkfs6y6xxwrtticnzqxhf/envs/BioTokens-cpu/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = TransformerNet(ENG_VOCAB_SIZE, DEU_VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_SIZE, NUM_HEADS, NUM_LAYERS, MAX_SENT_LEN, MAX_SENT_LEN, DROPOUT).to(DEVICE)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "value cannot be converted to type uint8_t without overflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:6\u001b[0m\n",
      "File \u001b[0;32m~/spack/opt/spack/linux-centos7-x86_64_v3/gcc-4.8.5/anaconda3-2022.10-ldbv2ghkumdvkfs6y6xxwrtticnzqxhf/envs/BioTokens-cpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/spack/opt/spack/linux-centos7-x86_64_v3/gcc-4.8.5/anaconda3-2022.10-ldbv2ghkumdvkfs6y6xxwrtticnzqxhf/envs/BioTokens-cpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/postgrads/2650807G/BioToken/test.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226275636b6574686561642e656e672e676c612e61632e756b222c2275736572223a223236353038303747227d/home/postgrads/2650807G/BioToken/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m src, tgt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc_pe(src), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdec_pe(tgt)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226275636b6574686561642e656e672e676c612e61632e756b222c2275736572223a223236353038303747227d/home/postgrads/2650807G/BioToken/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(src)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226275636b6574686561642e656e672e676c612e61632e756b222c2275736572223a223236353038303747227d/home/postgrads/2650807G/BioToken/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m transformer_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(tgt, memory, mask_tgt)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226275636b6574686561642e656e672e676c612e61632e756b222c2275736572223a223236353038303747227d/home/postgrads/2650807G/BioToken/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m final_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(transformer_out)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226275636b6574686561642e656e672e676c612e61632e756b222c2275736572223a223236353038303747227d/home/postgrads/2650807G/BioToken/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_softmax(final_out)\n",
      "File \u001b[0;32m~/spack/opt/spack/linux-centos7-x86_64_v3/gcc-4.8.5/anaconda3-2022.10-ldbv2ghkumdvkfs6y6xxwrtticnzqxhf/envs/BioTokens-cpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/spack/opt/spack/linux-centos7-x86_64_v3/gcc-4.8.5/anaconda3-2022.10-ldbv2ghkumdvkfs6y6xxwrtticnzqxhf/envs/BioTokens-cpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/spack/opt/spack/linux-centos7-x86_64_v3/gcc-4.8.5/anaconda3-2022.10-ldbv2ghkumdvkfs6y6xxwrtticnzqxhf/envs/BioTokens-cpu/lib/python3.11/site-packages/torch/nn/modules/transformer.py:457\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    454\u001b[0m output \u001b[39m=\u001b[39m tgt\n\u001b[1;32m    456\u001b[0m seq_len \u001b[39m=\u001b[39m _get_seq_len(tgt, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mself_attn\u001b[39m.\u001b[39mbatch_first)\n\u001b[0;32m--> 457\u001b[0m tgt_is_causal \u001b[39m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[1;32m    459\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m    460\u001b[0m     output \u001b[39m=\u001b[39m mod(output, memory, tgt_mask\u001b[39m=\u001b[39mtgt_mask,\n\u001b[1;32m    461\u001b[0m                  memory_mask\u001b[39m=\u001b[39mmemory_mask,\n\u001b[1;32m    462\u001b[0m                  tgt_key_padding_mask\u001b[39m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    463\u001b[0m                  memory_key_padding_mask\u001b[39m=\u001b[39mmemory_key_padding_mask,\n\u001b[1;32m    464\u001b[0m                  tgt_is_causal\u001b[39m=\u001b[39mtgt_is_causal,\n\u001b[1;32m    465\u001b[0m                  memory_is_causal\u001b[39m=\u001b[39mmemory_is_causal)\n",
      "File \u001b[0;32m~/spack/opt/spack/linux-centos7-x86_64_v3/gcc-4.8.5/anaconda3-2022.10-ldbv2ghkumdvkfs6y6xxwrtticnzqxhf/envs/BioTokens-cpu/lib/python3.11/site-packages/torch/nn/modules/transformer.py:921\u001b[0m, in \u001b[0;36m_detect_is_causal_mask\u001b[0;34m(mask, is_causal, size)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[39mif\u001b[39;00m is_causal \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     sz \u001b[39m=\u001b[39m size \u001b[39mif\u001b[39;00m size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m mask\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 921\u001b[0m     causal_comparison \u001b[39m=\u001b[39m _generate_square_subsequent_mask(\n\u001b[1;32m    922\u001b[0m         sz, device\u001b[39m=\u001b[39;49mmask\u001b[39m.\u001b[39;49mdevice, dtype\u001b[39m=\u001b[39;49mmask\u001b[39m.\u001b[39;49mdtype)\n\u001b[1;32m    924\u001b[0m     \u001b[39m# Do not use `torch.equal` so we handle batched masks by\u001b[39;00m\n\u001b[1;32m    925\u001b[0m     \u001b[39m# broadcasting the comparison.\u001b[39;00m\n\u001b[1;32m    926\u001b[0m     \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m causal_comparison\u001b[39m.\u001b[39msize():\n",
      "File \u001b[0;32m~/spack/opt/spack/linux-centos7-x86_64_v3/gcc-4.8.5/anaconda3-2022.10-ldbv2ghkumdvkfs6y6xxwrtticnzqxhf/envs/BioTokens-cpu/lib/python3.11/site-packages/torch/nn/modules/transformer.py:27\u001b[0m, in \u001b[0;36m_generate_square_subsequent_mask\u001b[0;34m(sz, device, dtype)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_square_subsequent_mask\u001b[39m(\n\u001b[1;32m     19\u001b[0m         sz: \u001b[39mint\u001b[39m,\n\u001b[1;32m     20\u001b[0m         device: torch\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_default_device()),  \u001b[39m# torch.device('cpu'),\u001b[39;00m\n\u001b[1;32m     21\u001b[0m         dtype: torch\u001b[39m.\u001b[39mdtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mget_default_dtype(),\n\u001b[1;32m     22\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     23\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m        Unmasked positions are filled with float(0.0).\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtriu(\n\u001b[0;32m---> 27\u001b[0m         torch\u001b[39m.\u001b[39;49mfull((sz, sz), \u001b[39mfloat\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m-inf\u001b[39;49m\u001b[39m'\u001b[39;49m), dtype\u001b[39m=\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49mdevice),\n\u001b[1;32m     28\u001b[0m         diagonal\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     29\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: value cannot be converted to type uint8_t without overflow"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss_trace = []\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "  current_loss = 0\n",
    "  for i, (x, y) in enumerate(train_loader):\n",
    "    x, y  = x.to(DEVICE), y.to(DEVICE)\n",
    "    outputs = model(x, y)\n",
    "    loss = criterion(outputs.permute(1, 2, 0), y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    current_loss += loss.item()\n",
    "  loss_trace.append(current_loss)\n",
    "torch.save(model, 'mymodel.pt')\n",
    "\n",
    "# loss curve\n",
    "plt.plot(range(1, NUM_EPOCHS+1), loss_trace, 'r-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = 0, 0\n",
    "predictions = []\n",
    "lmatch = 0\n",
    "model.eval()\n",
    "for i, (x, y) in enumerate(test_loader):\n",
    "    with torch.no_grad():\n",
    "        x, y  = x.to(DEVICE), y.to(DEVICE)\n",
    "        outputs = model(x,y)\n",
    "        _, y_pred = torch.max(outputs.permute(1,2,0).data, 1)\n",
    "        for j in range(y.shape[0]):\n",
    "            input_x =  [eng_words[k] for k in x[j]]\n",
    "            target_y = [deu_words[k] for k in y[j]]\n",
    "            output_y = [deu_words[k] for k in y_pred[j]]\n",
    "            print(input_x)\n",
    "            print(target_y)\n",
    "            print(output_y)\n",
    "            if len(target_y) == len(output_y):\n",
    "                for u in range(len(target_y)):\n",
    "                    if target_y[u] == '<eos>':\n",
    "                        break\n",
    "                    total += 1\n",
    "                    if target_y[u] == output_y[u]:\n",
    "                        correct += 1\n",
    "            else:\n",
    "                lmatch += 1\n",
    "\n",
    "print(correct/total) \n",
    "print(lmatch) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = 0, 0\n",
    "predictions = []\n",
    "for i, (x,y) in enumerate(test_loader):\n",
    "  with torch.no_grad():\n",
    "    x, y  = x.to(DEVICE), y.to(DEVICE)\n",
    "    outputs = model(x,y)\n",
    "    _, y_pred = torch.max(outputs.permute(1,2,0).data, 1)\n",
    "    print(y[0])\n",
    "    print(y_pred[0])\n",
    "    total += y.shape[0]*y.shape[1]\n",
    "    correct += (y_pred == y).sum().item()\n",
    "\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "model_scripted.save('mymodel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioTokens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
